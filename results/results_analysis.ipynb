{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import xarray as xr\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import metrics\n",
    "sys.path.append(os.path.realpath('../split-data/'))\n",
    "import return_period_tools as tools\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.16.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./model_output_for_analysis/nwm_chrt_v2_1d_local.p', 'rb') as fb:\n",
    "with open('./nwm_chrt_v2_1d_local.p', 'rb') as fb:\n",
    "    nwm_results = pkl.load(fb)\n",
    "\n",
    "lstm_results_time_split1={}\n",
    "mclstm_results_time_split1={}\n",
    "sacsma_results_time_split1={}\n",
    "lstm_results_time_split2={}\n",
    "mclstm_results_time_split2={}\n",
    "sacsma_results_time_split2={}\n",
    "lstm_results_return_period_split={}\n",
    "mclstm_results_return_period_split={}\n",
    "sacsma_results_return_period_split={}\n",
    "\n",
    "for forcing_type in ['nldas', 'daymet']:\n",
    "    \n",
    "    with open('./model_output_for_analysis/lstm_time_split1_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        lstm_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/mclstm_time_split1_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        mclstm_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/sacsma_time_split1_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        sacsma_results_time_split1[forcing_type] = pkl.load(fb)\n",
    "\n",
    "    with open('./model_output_for_analysis/lstm_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        lstm_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/mclstm_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        mclstm_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/sacsma_time_split2_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        sacsma_results_time_split2[forcing_type] = pkl.load(fb)\n",
    "\n",
    "    with open('./model_output_for_analysis/lstm_return_period_split_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        lstm_results_return_period_split[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/mclstm_return_period_split_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        mclstm_results_return_period_split[forcing_type] = pkl.load(fb)\n",
    "    with open('./model_output_for_analysis/sacsma_return_period_split_{}.p'.format(forcing_type), 'rb') as fb:\n",
    "        sacsma_results_return_period_split[forcing_type] = pkl.load(fb)\n",
    "\n",
    "train_split_type_model_set = {'time_split1':{'nwm':nwm_results, \n",
    "                                           'lstm':lstm_results_time_split1,\n",
    "                                            'mc':mclstm_results_time_split1,\n",
    "                                            'sac':sacsma_results_time_split1},\n",
    "                              'time_split2':{'nwm':nwm_results, \n",
    "                                           'lstm':lstm_results_time_split2,\n",
    "                                            'mc':mclstm_results_time_split2,\n",
    "                                            'sac':sacsma_results_time_split2},\n",
    "                              'return_period_split':{'nwm':nwm_results, \n",
    "                                           'lstm':lstm_results_return_period_split,\n",
    "                                            'mc':mclstm_results_return_period_split,\n",
    "                                            'sac':sacsma_results_return_period_split}}\n",
    "\n",
    "range_for_analysis = {'time_split1': [1989,1999],'time_split2': [1996, 2014],'return_period_split':[1996, 2014]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert flow to   CFS mm -> ft     km^2 -> ft^2    hr->s\n",
    "conversion_factor = 0.00328084 * 10763910.41671 / 3600 / 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the basins in the analysis\n",
    "#basin_list = list(lstm_results_time_split.keys())\n",
    "\n",
    "# Camels attributes with RI information\n",
    "dataName = '../data/camels_attributes.csv'\n",
    "# load the data with pandas\n",
    "pd_attributes = pd.read_csv(dataName, sep=',', index_col='gauge_id')\n",
    "\n",
    "# Add the basin ID as a 8 element string with a leading zero if neccessary\n",
    "basin_id_str = []\n",
    "for a in pd_attributes.index.values:\n",
    "    basin_id_str.append(str(a).zfill(8))\n",
    "pd_attributes['basin_id_str'] = basin_id_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all the SACSMA runs and check that the results are good. \n",
    "# Get a list of basins that has good calibration results.\n",
    "\n",
    "basin_list_all_camels = list(pd_attributes['basin_id_str'].values)\n",
    "basin_list = copy.deepcopy(basin_list_all_camels)\n",
    "\n",
    "for ib, basin_0str in enumerate(basin_list_all_camels): \n",
    "    remove_basin_id_from_list = False\n",
    "    for train_split_type in ['time_split1', 'time_split2', 'return_period_split']:\n",
    "        for forcing_type in ['nldas', 'daymet']:\n",
    "            if basin_0str not in list(train_split_type_model_set[train_split_type]['sac'][forcing_type].columns):\n",
    "                remove_basin_id_from_list = True\n",
    "            elif train_split_type_model_set[train_split_type]['sac'][forcing_type][basin_0str].sum() <=0:\n",
    "                remove_basin_id_from_list = True\n",
    "                \n",
    "    if remove_basin_id_from_list:\n",
    "        basin_list.remove(basin_0str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "# Solve this problem. I think it is the xarray structures...\n",
    "# isibleDeprecationWarning: Creating an ndarray from ragged nested sequences \n",
    "# (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. \n",
    "# If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics_for_frequency_analysis(analysis_dict, flows, recurrance_interval):\n",
    "\n",
    "    sims = list(flows.keys())[:-1]\n",
    "\n",
    "    for metric in metrics.get_available_metrics():\n",
    "\n",
    "        score = {sim:0 for sim in sims}\n",
    "    \n",
    "        analysis_dict[metric]['ri'].append(recurrance_interval)\n",
    "    \n",
    "        if metric == 'NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.nse(flows['obs'],flows[sim])\n",
    "        if metric == 'MSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.mse(flows['obs'],flows[sim])\n",
    "        if metric == 'RMSE':\n",
    "            for sim in sims:\n",
    "                 score[sim] = metrics.rmse(flows['obs'],flows[sim])\n",
    "        if metric == 'KGE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.kge(flows['obs'],flows[sim])\n",
    "        if metric == 'Alpha-NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.alpha_nse(flows['obs'],flows[sim])\n",
    "        if metric == 'Beta-NSE':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.beta_nse(flows['obs'],flows[sim])\n",
    "        if metric == 'Pearson-r':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.pearsonr(flows['obs'],flows[sim])\n",
    "        if metric == 'Peak-Timing':\n",
    "            for sim in sims:\n",
    "                score[sim] = np.abs(metrics.mean_peak_timing(flows['obs'],flows[sim]))\n",
    "        if metric == 'FHV':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_fhv(flows['obs'],flows[sim])\n",
    "        if metric == 'FLV':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_flv(flows['obs'],flows[sim])\n",
    "        if metric == 'FMS':\n",
    "            for sim in sims:\n",
    "                score[sim] = metrics.fdc_fms(flows['obs'],flows[sim])\n",
    "\n",
    "        for sim in sims:\n",
    "            analysis_dict[metric][sim].append(score[sim])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing  nldas\n",
      "    Analyzing  return_period_split\n"
     ]
    }
   ],
   "source": [
    "# flows = ['nwm', 'lstm', 'mc', 'sac', 'obs']\n",
    "# for forcing_type in ['nldas', 'daymet']:\n",
    "flows = ['lstm','obs']\n",
    "for forcing_type in ['nldas']:\n",
    "    print('Analyzing ',forcing_type)\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    analysis_dict_names = {'time_split1':'frequency_analysis_dict_time_split1_{}.pkl'.format(forcing_type),\n",
    "                           'time_split2':'frequency_analysis_dict_time_split2_{}.pkl'.format(forcing_type), \n",
    "                          'return_period_split':'frequency_analysis_dict_return_period_split_{}.pkl'.format(forcing_type)}\n",
    "    peak_flows_dict_names = {'time_split1':'peak_annual_flows_dict_time_split1_{}.pkl'.format(forcing_type),\n",
    "                             'time_split2':'peak_annual_flows_dict_time_split2_{}.pkl'.format(forcing_type),\n",
    "                             'return_period_split':'peak_annual_flows_dict_return_period_split_{}.pkl'.format(forcing_type)}\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "    #----------   If the calcs have been done, then just read them in.\n",
    "    if False:\n",
    "        with open(analysis_dict_names[train_split_type], 'rb') as fb:\n",
    "            analysis_dict_all = pkl.load(fb)\n",
    "        with open(peak_flows_dict_names[train_split_type], 'rb') as fb:\n",
    "            peak_flows_dict = pkl.load(fb)\n",
    "            \n",
    "    else:\n",
    "\n",
    "        #for train_split_type in ['time_split1', 'time_split2', 'return_period_split']:\n",
    "        for train_split_type in ['return_period_split']:\n",
    "            print('    Analyzing ',train_split_type)\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Set up lists\n",
    "            if train_split_type == 'time_split1':\n",
    "                models = ['lstm', 'mc', 'sac']\n",
    "                flows = ['lstm', 'mc', 'sac', 'obs']\n",
    "                models_obs_ri = ['lstm', 'mc', 'sac', 'obs', 'ri']\n",
    "                models_ri = ['lstm', 'mc', 'sac', 'ri']\n",
    "            else:\n",
    "                models = ['nwm', 'lstm', 'mc', 'sac']\n",
    "                flows = ['nwm', 'lstm', 'mc', 'sac', 'obs']\n",
    "                models_obs_ri = ['nwm', 'lstm', 'mc', 'sac', 'obs', 'ri']\n",
    "                models_ri = ['nwm', 'lstm', 'mc', 'sac', 'ri']\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            # Place the data here\n",
    "            analysis_dict_all = {}\n",
    "            peak_flows_dict = {i:[] for i in models_obs_ri}\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            #-----LOOP THROUGH BASINS------------------------------------------------------------------------\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "            for ib, basin_0str in enumerate(basin_list): \n",
    "                basin_int = int(basin_0str)\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Get the NWM data for this basin in an xarray dataset.\n",
    "                xr_nwm = xr.DataArray(train_split_type_model_set[train_split_type]['nwm'][basin_0str]['streamflow'].values, \n",
    "                         coords=[nwm_results[basin_0str]['streamflow'].index], \n",
    "                         dims=['datetime'])\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Setting up the dictionary for the single basin results. Then will add to the overall dict.\n",
    "                analysis_dict = {metric:{model:[] for model in models_ri} for metric in metrics.get_available_metrics()}\n",
    "                extra_metrics = ['beta-abs', 'peakQ', 'peakRI', 'peakT', 'peakQ-perc', 'peakRI-perc', 'peakT-abs']\n",
    "                for extra_metric in extra_metrics:\n",
    "                    analysis_dict[extra_metric] = {model:[] for model in models_ri}\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # We need the basin area to convert to CFS, to interpolate the RI from LPIII\n",
    "                basin_area = pd_attributes.loc[basin_int, 'area_geospa_fabric']\n",
    "                basin_str = tools.gauge_id_str(basin_int)\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Get the log pearson III results\n",
    "                b17 = tools.read_b17(basin_str)\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Get the peak flows, but then cut them to just the validation year.\n",
    "                peakflows = tools.read_peak_flows(basin_str)\n",
    "                peakflows['wateryear'] = [int(tools.get_water_year(int(peakflows.iloc[i,0].split('-')[0]), \n",
    "                                          int(peakflows.iloc[i,0].split('-')[1]))) for i in range(peakflows.shape[0])]\n",
    "                peakflows = pd.DataFrame(peakflows.set_index('wateryear'))\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                #----  LOOP THROUGH THE WATER YEARS   ------------------------------------------------------------\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                for water_year in range(range_for_analysis[train_split_type][0], \n",
    "                                        range_for_analysis[train_split_type][1]):\n",
    "                    date_from = str(water_year-1)+'-10'\n",
    "                    date_to = str(water_year)+'-09'\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Figure out what the actual recurrence interval is for the basin-year. \n",
    "                    # We'll use this to categorize the basin-year, but then calc the metrics with the observations.\n",
    "                    if water_year not in list(peakflows.index.values):\n",
    "                        #print(\"water year not in record\")\n",
    "                        continue\n",
    "                    peak_date = peakflows.loc[water_year, 0]\n",
    "                    if isinstance(peakflows.loc[water_year, 1], str):\n",
    "                        peak_flow = float(peakflows.loc[water_year, 1].replace(\" \", \"\"))\n",
    "                    else:\n",
    "                        peak_flow = peakflows.loc[water_year, 1]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Make dictionary with all the flows\n",
    "                    flow_mm = {}\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------        \n",
    "                     # NWM data\n",
    "                    if train_split_type != 'time_split1':\n",
    "                        sim_nwm = xr_nwm.loc[date_from:date_to]\n",
    "                        # convert from CFS to mm/day\n",
    "                        # fm3/s * 3600 sec/hour * 24 hour/day / (m2 * mm/m)\n",
    "                        flow_mm['nwm'] = sim_nwm*3600*24/(basin_area*1000)\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Standard LSTM \n",
    "                    xrr = train_split_type_model_set[train_split_type]['lstm'][forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "                    flow_mm['lstm'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Mass-conserving LSTM data trained on all years\n",
    "                    xrr = train_split_type_model_set[train_split_type]['mc'][forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim']\n",
    "                    flow_mm['mc'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------        \n",
    "                    # SACSMA \n",
    "                    df = train_split_type_model_set[train_split_type]['sac'][forcing_type][basin_0str]\n",
    "                    flow_mm['sac'] = df.loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # OBSERVATIONS\n",
    "                    xrr = train_split_type_model_set[train_split_type]['mc'][forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs']\n",
    "                    flow_mm['obs'] = pd.DataFrame(data=xrr.values,index=xrr.date.values).loc[date_from:date_to]\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Get the max flow and the location of the max flow. Make them all of same type (np.array).\n",
    "                    max_flow_mm = {flow:np.array(flow_mm[flow].max()) for flow in flows}\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Skip basin years that do not have data\n",
    "                    if np.isnan(max_flow_mm['obs']):\n",
    "                        #print('no data, skipping')\n",
    "                        continue\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Get the location of the max flow within the year. If cannot get this value, then need to skip yr\n",
    "                    try:\n",
    "                        max_loc = flow_mm['obs'].values.argmax()\n",
    "                    except:\n",
    "                        #print('cannot find max loc')\n",
    "                        continue\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Set the event window around the peak flow\n",
    "                    event_window_days = 10\n",
    "                    max_event_start = pd.Timestamp(date_from) + pd.Timedelta(max_loc - event_window_days, 'day')\n",
    "                    max_event_end = pd.Timestamp(date_from) + pd.Timedelta(max_loc + event_window_days, 'day')\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Get the peak flows within the event window\n",
    "                    max_flow_cfs={}\n",
    "                    for iflow in flows:\n",
    "                        max_flow_cfs[iflow] = np.min([np.max([max_flow_mm[iflow] * \\\n",
    "                                                              basin_area * conversion_factor, 0]),b17.max()])\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Calculate the recurrence interval of the maximum annual flow from the log pearson III\n",
    "                    ri={}\n",
    "                    ri['ri'] = tools.interpolate_ri(peak_flow, b17)\n",
    "                    for iflow in flows:\n",
    "                        ri[iflow] = tools.interpolate_ri(max_flow_cfs[iflow], b17)\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # The recurrence interval will be None if the flow is too low.\n",
    "                    # We can skip over flows that are too low, because we have plenty of those in our sample\n",
    "                    bad_ri_value_continue = False\n",
    "                    for iflow in ri.keys():\n",
    "                        if not ri[iflow]:\n",
    "                            #print('RI is less than 1')\n",
    "                            bad_ri_value_continue = True\n",
    "                            break\n",
    "                    if bad_ri_value_continue:\n",
    "                        continue\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Get all the peak flow values into one place.\n",
    "                    for iflow in flows:\n",
    "                        peak_flows_dict[iflow].append(np.max([max_flow_mm[iflow]]))\n",
    "                    peak_flows_dict['ri'].append(ri['obs'])\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Make all xarray data similar\n",
    "                    for iflow in flows:\n",
    "                        if iflow == 'nwm': #already in the correct format\n",
    "                            continue\n",
    "                        if iflow == 'sac': \n",
    "                            flow_mm[iflow] = xr.DataArray(np.array(flow_mm[iflow].values, dtype='float32'), \n",
    "                                           coords=dict(datetime=flow_mm[iflow].index.values), dims=['datetime'])\n",
    "                        else:\n",
    "                            flow_mm[iflow] = xr.DataArray(flow_mm[iflow].values[:,0], \n",
    "                                           coords=dict(datetime=flow_mm[iflow].index.values), dims=['datetime'])\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    if flow_mm['mc'].sum() > 0 and flow_mm['obs'].sum() > 0:\n",
    "                        # This function loops through the metrics and calculates them.\n",
    "                        calculate_all_metrics_for_frequency_analysis(analysis_dict, flow_mm, ri['ri'])\n",
    "                    else:\n",
    "                        continue\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    # Calculate the 'simple' flow and timing error. Just peak differences.\n",
    "                    for extra_metric in extra_metrics:\n",
    "                        analysis_dict[extra_metric]['ri'].append(ri['ri'])\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    for iflow in models:\n",
    "                        max_obs_loc = flow_mm['obs'].argmax().values\n",
    "                        sim_max_window = flow_mm[iflow][np.max([0,max_obs_loc-10]):np.min([365,max_obs_loc+10])]\n",
    "                        max_sim_loc = max_obs_loc + (sim_max_window.argmax().values - 10)\n",
    "                        analysis_dict['peakT'][iflow].append(max_sim_loc - max_obs_loc)\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    for iflow in models:\n",
    "                        analysis_dict['peakRI'][iflow].append(ri[iflow] - ri['obs'])\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    for iflow in models:\n",
    "                        analysis_dict['peakRI-perc'][iflow].append(np.abs(ri[iflow] - ri['obs'])/ri['obs'])\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    for iflow in models:\n",
    "                        analysis_dict['peakQ'][iflow].append((flow_mm[iflow].max().values - flow_mm['obs'].max().values) * \\\n",
    "                                                                 basin_area * conversion_factor)\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    for iflow in models:\n",
    "                        analysis_dict['peakQ-perc'][iflow].append(np.abs(flow_mm[iflow].max().values - \\\n",
    "                                                                         flow_mm['obs'].max().values) / \\\n",
    "                                                                         flow_mm['obs'].max().values)\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "                    for iflow in models:\n",
    "                        max_obs_loc = flow_mm['obs'].argmax().values\n",
    "                        sim_max_window = flow_mm[iflow][np.max([0,max_obs_loc-10]):np.min([365,max_obs_loc+10])]\n",
    "                        max_sim_loc = max_obs_loc + (sim_max_window.argmax().values - 10)\n",
    "                        analysis_dict['peakT-abs'][iflow].append(np.abs(max_sim_loc - max_obs_loc))\n",
    "                    #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    #------------------------------------------------------------------------------------------------- \n",
    "                    for iflow in models:\n",
    "                        analysis_dict['beta-abs'][iflow].append(np.abs(analysis_dict['Beta-NSE'][iflow][-1]))\n",
    "                    #-------------------------------------------------------------------------------------------------        \n",
    "\n",
    "                    #------------------------------------------------------------------------------------------------- \n",
    "                    if train_split_type == 'hp' and ri['ri'] > 50:\n",
    "                        print('recurrance_interval:', ri['ri'])\n",
    "                        for iflow in flows:\n",
    "                            plt.plot(flow_mm[iflow], label=iflow)\n",
    "                        plt.title('{}, {}'.format(basin_0str, water_year))\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "                        plt.close()\n",
    "                    #------------------------------------------------------------------------------------------------- \n",
    "\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                #Now that the basin has been analyzed successfully, add it to the larger dictionary\n",
    "                analysis_dict_all[basin_0str] = analysis_dict\n",
    "                #------------------------------------------------------------------------------------------------- \n",
    "\n",
    "            #-------------------------------------------------------------------------------------------------\n",
    "            with open(analysis_dict_names[train_split_type], 'wb') as fb:\n",
    "                pkl.dump(analysis_dict_all, fb)\n",
    "            with open(peak_flows_dict_names[train_split_type], 'wb') as fb:\n",
    "                pkl.dump(peak_flows_dict, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = ['nwm', 'lstm', 'mc', 'sac', 'obs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing  daymet\n",
      "Analyzing  nldas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name_map = {'nldas':'nldas', 'daymet':'cida'}\n",
    "precip_column_map = {'nldas':'PRCP(mm/day)', 'daymet':'prcp(mm/day)'}\n",
    "\n",
    "for forcing_type in ['daymet','nldas']:\n",
    "\n",
    "    print('Analyzing ',forcing_type)\n",
    "    \n",
    "    mass_basin_list = []\n",
    "    \n",
    "    forcing_dir = '/home/NearingLab/data/camels_data/basin_dataset_public_v1p2/basin_mean_forcing/{}_all_basins_in_one_directory/'.format(forcing_type)\n",
    "    \n",
    "    total_mass_error = {'absolute':{flow:[] for flow in flows}, \n",
    "                  'positive':{flow:[] for flow in flows}, \n",
    "                  'negative':{flow:[] for flow in flows}}\n",
    "\n",
    "    total_mass = {}\n",
    "\n",
    "    cumulative_mass_all = {}\n",
    "\n",
    "    labelz={'nwm':'NWM*', 'lstm':'LSTM', 'mc':'MC-LSTM','sac':'SAC-SMA', 'obs':'Observed'}\n",
    "\n",
    "    start_date = pd.Timestamp('1996-10-01')\n",
    "    end_date = pd.Timestamp('2014-01-01')\n",
    "\n",
    "    first_basin = True\n",
    "\n",
    "    for basin_0str in basin_list:\n",
    "        basin_int = int(basin_0str)\n",
    "#        print(basin_0str)\n",
    "\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # Reset the total mass to zero for this basin    \n",
    "        cumulative_mass = {flow:[0] for flow in flows}\n",
    "        cumulative_mass['precip'] = [0]\n",
    "        total_mass[basin_0str] = {flow:0 for flow in flows}\n",
    "        imass=1\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # We need the basin area to convert to CFS, to interpolate the RI from LPIII\n",
    "        basin_area = pd_attributes.loc[basin_int, 'area_geospa_fabric']\n",
    "        basin_str = tools.gauge_id_str(basin_int)\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # Make dictionary with all the flows\n",
    "        flow_mm = {}    \n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # Get the NWM data for this basin in an xarray dataset.\n",
    "        xr_nwm = xr.DataArray(nwm_results[basin_0str]['streamflow'].values, \n",
    "                 coords=[nwm_results[basin_0str]['streamflow'].index], \n",
    "                 dims=['datetime'])\n",
    "        # convert from CFS to mm/day\n",
    "        # fm3/s * 3600 sec/hour * 24 hour/day / (m2 * mm/m)\n",
    "        flow_mm['nwm'] = xr_nwm.loc[start_date:end_date]*3600*24/(basin_area*1000)\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # Standard LSTM \n",
    "        xrr = lstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim'].loc[start_date:end_date]\n",
    "        flow_mm['lstm'] = pd.DataFrame(data=xrr.values,index=xrr.date.values)\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # Mass-conserving LSTM data trained on all years\n",
    "        xrr = mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_sim'].loc[start_date:end_date]\n",
    "        flow_mm['mc'] = pd.DataFrame(data=xrr.values,index=xrr.date.values)\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # SACSMA Mean\n",
    "        df = sacsma_results_time_split2[forcing_type][basin_0str].loc[start_date:end_date]\n",
    "        flow_mm['sac'] = df\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # OBSERVATIONS\n",
    "        xrr = mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs'].loc[start_date:end_date]\n",
    "        flow_mm['obs'] = pd.DataFrame(data=xrr.values,index=xrr.date.values)\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # FORCING\n",
    "        forcing = pd.read_csv(forcing_dir+basin_0str+'_lump_{}_forcing_leap.txt'.format(file_name_map[forcing_type]), delim_whitespace=True, header=3)\n",
    "        forcing = forcing.iloc[6118:]\n",
    "        forcing.index=pd.to_datetime((forcing.Year*10000+forcing.Mnth*100+forcing.Day).apply(str),format='%Y%m%d')\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # Make sure we are in a time period that all the flow members have values\n",
    "        # If there is missin observations than we can't compare the mass of the observed with simulaitons\n",
    "        skip_basin_because_missing_obs = False\n",
    "        for d in mclstm_results_time_split2[forcing_type][basin_0str]['1D']['xr']['QObs(mm/d)_obs'].date:\n",
    "            if d.values < start_date:\n",
    "                continue\n",
    "            if d.values > end_date:\n",
    "                break\n",
    "            if np.isnan(flow_mm['obs'].loc[d.values].values[0]):\n",
    "                skip_basin_because_missing_obs = True\n",
    "                break\n",
    "            else:\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "                # Keep track of the cumulative mass and add it to the list\n",
    "                cumulative_mass['precip'].append(forcing[precip_column_map[forcing_type]].loc[d.values] + \\\n",
    "                                                 cumulative_mass['precip'][imass-1])\n",
    "\n",
    "                cumulative_mass['obs'].append(flow_mm['obs'].loc[d.values].values[0] + \\\n",
    "                                              cumulative_mass['obs'][imass-1])\n",
    "\n",
    "                cumulative_mass['nwm'].append(flow_mm['nwm'].loc[d.values].values + \\\n",
    "                                              cumulative_mass['nwm'][imass-1])\n",
    "\n",
    "                cumulative_mass['lstm'].append(flow_mm['lstm'].loc[d.values].values[0] + \\\n",
    "                                               cumulative_mass['lstm'][imass-1])\n",
    "\n",
    "                cumulative_mass['mc'].append(flow_mm['mc'].loc[d.values].values[0] + \\\n",
    "                                             cumulative_mass['mc'][imass-1])\n",
    "\n",
    "                cumulative_mass['sac'].append(flow_mm['sac'].loc[d.values] + \\\n",
    "                                              cumulative_mass['sac'][imass-1])\n",
    "                imass+=1\n",
    "                #-------------------------------------------------------------------------------------------------\n",
    "\n",
    "        #-------------------------------------------------------------------------------------------------\n",
    "        # If there is missin observations than we can't compare the mass of the observed with simulaitons            \n",
    "        if skip_basin_because_missing_obs:\n",
    "#            print(\"skipping basin {} because of missing observations\".format(basin_0str))\n",
    "            continue\n",
    "        else:\n",
    "            mass_basin_list.append(basin_0str)\n",
    "    \n",
    "        for flow in flows:\n",
    "            total_mass[basin_0str][flow] = np.nansum(flow_mm[flow].loc[start_date:end_date])\n",
    "\n",
    "        for flow in flows:\n",
    "            total_mass_error['absolute'][flow].append(np.abs(total_mass[basin_0str][flow] - \\\n",
    "                                                             total_mass[basin_0str]['obs'])/ \\\n",
    "                                                             total_mass[basin_0str]['obs'])\n",
    "            if (total_mass[basin_0str][flow] - total_mass[basin_0str]['obs']) > 0:\n",
    "                total_mass_error['positive'][flow].append((total_mass[basin_0str][flow] - \\\n",
    "                                                           total_mass[basin_0str]['obs'])/ \\\n",
    "                                                           total_mass[basin_0str]['obs'])\n",
    "                total_mass_error['negative'][flow].append(0)\n",
    "            else:\n",
    "                total_mass_error['negative'][flow].append((total_mass[basin_0str][flow] - \\\n",
    "                                                           total_mass[basin_0str]['obs']) / \\\n",
    "                                                           total_mass[basin_0str]['obs'])\n",
    "                total_mass_error['positive'][flow].append(0)\n",
    "\n",
    "        # _______________________________________________________________________\n",
    "        # Keep track of all the cumulative mass through time for each basin\n",
    "        if first_basin and not skip_basin_because_missing_obs:\n",
    "            for flow in flows:\n",
    "                cumulative_mass_all[flow] = np.array(cumulative_mass[flow])\n",
    "            cumulative_mass_all['precip'] = np.array(cumulative_mass['precip'])\n",
    "            first_basin = False\n",
    "        if  not skip_basin_because_missing_obs and not first_basin:\n",
    "            for flow in flows:\n",
    "                cumulative_mass_all[flow] += np.array(cumulative_mass[flow])\n",
    "            cumulative_mass_all['precip'] +=np.array(cumulative_mass['precip'])\n",
    "\n",
    "    # _______________________________________________________________________\n",
    "    # Save the mass balance results.\n",
    "    with open('total_mass_error_{}.pkl'.format(forcing_type), 'wb') as fb:\n",
    "        pkl.dump(total_mass_error, fb)\n",
    "    with open('total_mass_{}.pkl'.format(forcing_type), 'wb') as fb:\n",
    "        pkl.dump(total_mass, fb)\n",
    "    with open('cumulative_mass_all_{}.pkl'.format(forcing_type), 'wb') as fb:\n",
    "        pkl.dump(cumulative_mass_all, fb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mass_basin_list.pkl', 'wb') as fb:\n",
    "    pkl.dump(mass_basin_list, fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
